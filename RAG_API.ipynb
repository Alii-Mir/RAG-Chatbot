{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Mount Google Drive"
      ],
      "metadata": {
        "id": "NYL2c7RRbx5F"
      },
      "id": "NYL2c7RRbx5F"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "68279a23",
      "metadata": {
        "id": "68279a23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36ef12c2-62c8-4503-c9af-15554b80a6c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# â­ï¸ PDF-Based RAG Q&A ChatBot Using API"
      ],
      "metadata": {
        "id": "Orf7w1_LpxYP"
      },
      "id": "Orf7w1_LpxYP"
    },
    {
      "cell_type": "code",
      "source": [
        "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
        "# â•‘ PDF Q&A RAG System with Groq Streaming LLM API â•‘\n",
        "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "# â”€â”€ STEP 0: Install Required Libraries â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "!pip -q install faiss-cpu langchain-core langchain-community \\\n",
        "               sentence-transformers groq gradio pydantic python-dotenv pypdf PyPDF2 torch\n",
        "\n",
        "# â”€â”€ STEP 1: Imports â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "import os, tempfile, traceback, torch\n",
        "from pathlib import Path\n",
        "from typing import List\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from groq import Groq\n",
        "import gradio as gr\n",
        "from PyPDF2 import PdfReader\n",
        "\n",
        "# â”€â”€ STEP 2: Configuration â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "GROQ_MODEL   = \"meta-llama/llama-4-scout-17b-16e-instruct\"\n",
        "EMBED_MODEL_NAME  = \"intfloat/e5-large-v2\"\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"ğŸ”‹ Using Device: {DEVICE}\")\n",
        "\n",
        "EMBEDDER = HuggingFaceEmbeddings(\n",
        "    model_name=EMBED_MODEL_NAME,\n",
        "    model_kwargs={\"device\": DEVICE}\n",
        ")\n",
        "\n",
        "# â”€â”€ STEP 3: Prompt Template for LLM â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "PROMPT_TEMPLATE = \"\"\"\n",
        "You are a highly knowledgeable assistant with full access to the contents of the provided PDF document(s).\n",
        "Your task is to answer the user's questions **strictly based on the content of the PDF**.\n",
        "If the answer cannot be found in the PDF, respond with: \"I do not know.\"\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "# â”€â”€ STEP 4: Build Vector Index from PDF Manual â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "def build_pdf_index(pdf_files: List):\n",
        "    \"\"\"\n",
        "    Builds a searchable vector index from uploaded PDFs.\n",
        "    Handles different types of file objects.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if not pdf_files:\n",
        "            return None, \"âŒ No PDF files uploaded.\"\n",
        "\n",
        "        docs = []\n",
        "        log_messages = []\n",
        "\n",
        "        for pdf in pdf_files:\n",
        "            log_message = \"\"\n",
        "            try:\n",
        "                # Determine if 'pdf' is a file-like object or a file path string.\n",
        "                if hasattr(pdf, \"read\"):\n",
        "                    # Read bytes directly (works for file-like streams)\n",
        "                    file_bytes = pdf.read()\n",
        "                    file_name = pdf.name if hasattr(pdf, \"name\") else \"uploaded.pdf\"\n",
        "                else:\n",
        "                    # If pdf is a path string, open it\n",
        "                    file_name = pdf\n",
        "                    with open(pdf, \"rb\") as f:\n",
        "                        file_bytes = f.read()\n",
        "\n",
        "                log_message += f\"ğŸ“„ Loading: {file_name}\\n\"\n",
        "\n",
        "                # Save the file bytes to a temporary file so PyPDFLoader can load it\n",
        "                with tempfile.NamedTemporaryFile(delete=False, suffix=\".pdf\") as tmp_pdf:\n",
        "                    tmp_pdf.write(file_bytes)\n",
        "                    tmp_pdf.flush()\n",
        "                    temp_path = tmp_pdf.name\n",
        "                    log_message += f\"\\nâœ… Saved to: {temp_path}\\n\"\n",
        "\n",
        "                    # Optional: Use PdfReader for debugging (e.g., getting a preview)\n",
        "                    try:\n",
        "                        reader = PdfReader(temp_path)\n",
        "                        num_pages = len(reader.pages)\n",
        "                        log_message += f\"\\nğŸ“‹ Pages Found: {num_pages}\\n\"\n",
        "                        if num_pages > 0:\n",
        "                            preview = reader.pages[0].extract_text()\n",
        "                            log_message += f\"\\nğŸ“ Page 1 Preview (A few characters):\\n{preview[:70]} ...\\n\\n\"\n",
        "                    except Exception as e:\n",
        "                        log_message += f\"âŒ PyPDF2 error: {e}\\n\"\n",
        "\n",
        "                    # Load the PDF using LangChain's PyPDFLoader\n",
        "                    loader = PyPDFLoader(temp_path)\n",
        "                    loaded_docs = loader.load()\n",
        "\n",
        "                    if not loaded_docs:\n",
        "                        log_message += f\"ğŸ›‘ No documents could be loaded from: {temp_path}\\n\"\n",
        "                    else:\n",
        "                        docs.extend(loaded_docs)\n",
        "                        log_message += f\"âœ… Successfully loaded {len(loaded_docs)} pages.\\n\"\n",
        "\n",
        "            except Exception as e:\n",
        "                log_message += f\"âŒ Error loading PDF: {traceback.format_exc()}\\n\"\n",
        "\n",
        "            log_messages.append(log_message)\n",
        "\n",
        "        if not docs:\n",
        "            return None, \"\\n\".join(log_messages) + \"\\nâŒ No documents were loaded. Check if the PDF is encrypted or corrupted.\"\n",
        "\n",
        "        # Split documents into chunks\n",
        "        splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)\n",
        "        chunks = splitter.split_documents(docs)\n",
        "        log_messages.append(f\"âœ… Created {len(chunks)} chunks for indexing.\\n\")\n",
        "\n",
        "        # Vectorize chunks\n",
        "        vdb = FAISS.from_documents(chunks, EMBEDDER)\n",
        "\n",
        "        log_messages.append(f\"âœ… Knowledge Base built with {len(chunks)} chunks.\")\n",
        "        full_log = \"\\n\".join(log_messages)\n",
        "\n",
        "        return vdb, full_log\n",
        "\n",
        "    except Exception as e:\n",
        "        err_msg = traceback.format_exc()\n",
        "        return None, f\"âŒ Error during indexing:\\n```\\n{err_msg}\\n```\"\n",
        "\n",
        "# â”€â”€ STEP 5: Gradio Interface â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "with gr.Blocks(title=\"PDF Q&A with Groq LLM\") as demo:\n",
        "    gr.Markdown(\"## ğŸ“„ PDF Q&A Assistant â€“ Ask Anything from Your PDF Documents\")\n",
        "\n",
        "    # Step 0: Enter API Key\n",
        "    api_key_input = gr.Textbox(label=\"ğŸ”‘ Enter your Groq API Key\", type=\"password\")\n",
        "\n",
        "    # Step 1: Upload and Index PDF\n",
        "    with gr.Row():\n",
        "        pdf_files = gr.Files(label=\"ğŸ“„ Upload Your PDF Files\")\n",
        "        build_button = gr.Button(\"ğŸ”§ Build Knowledge Base\")\n",
        "\n",
        "    # Scrollable logs\n",
        "    status = gr.Textbox(label=\"ğŸ“œ Logs and Errors\", interactive=False, lines=15, max_lines=30)\n",
        "\n",
        "    # Step 2: Ask a Question\n",
        "    with gr.Row():\n",
        "        question_input = gr.Textbox(label=\"ğŸ” Ask a Question\")\n",
        "        submit_button = gr.Button(\"ğŸš€ Get Answer\")\n",
        "\n",
        "    answer_output = gr.Textbox(label=\"ğŸ“ Answer\", interactive=False)\n",
        "\n",
        "    # Session state for the vector store\n",
        "    st_vdb = gr.State()\n",
        "\n",
        "    # Button Event Handlers\n",
        "    def build_and_store(pdfs):\n",
        "        vdb, log = build_pdf_index(pdfs)\n",
        "        return log, vdb\n",
        "\n",
        "    build_button.click(fn=build_and_store, inputs=[pdf_files], outputs=[status, st_vdb])\n",
        "    submit_button.click(fn=answer_question, inputs=[question_input, st_vdb, api_key_input], outputs=[answer_output])\n",
        "\n",
        "# â”€â”€ STEP 7: Launch Gradio App â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "demo.launch(share=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 628
        },
        "id": "994Aj8PI9N_w",
        "outputId": "6c6fc4e9-83ab-4a91-8870-b952d9685762"
      },
      "id": "994Aj8PI9N_w",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”‹ Using Device: cuda\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://2d390e49d854709028.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://2d390e49d854709028.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}